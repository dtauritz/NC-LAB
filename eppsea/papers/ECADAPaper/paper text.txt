Introduction
Typical population-based evolutionary algorithms (EAs) employ parent selection functions for selecting candidates for recombination as well as survival selection for reducing the size of the population or pool of future recombination candidates. To introduce selective pressure, these algorithms are often parameterized by an individual's fitness, generally in a sense that is relative to the population; in these functions, individuals with higher fitnesses are more preferable for selection. However, selection functions that prefer high-fitness individuals too strongly may lead the evolutionary search into a local optima. Thus, depending on the problem being solved and its fitness landscape, it may be desirable for selection functions to prefer individuals that initially seem less fit, on the chance that the individuals' genes will lead to more fit individuals in future generations. Because each problem has a different fitness landscape, careful tuning of selection functions is necessary to maximize performance of the EA. Selection functions are often chosen from a number of existing algorithms, and the parameters to these functions are tuned according to the problem of interest. Each algorithm causes the individuals' relative fitnesses to differently influence their chances of being selected. However, because there are an infinite number of ways an individual's fitness could be affected by its fitness, it is highly likely that an established selection algorithm is the optimal selection algorithm for a given problem.

Methodology
A meta-EA is used to evolve a high-performing parent selection function for a traditional population-based EA, with all other EA paramters (survival selection, population size, mutation rate and step size) kept constant. Individuals of the meta-EA represent selection functions, modeled as Koza-style GP-Trees which represent mathematical functions. The terminals of each function are parameters based on an individual's fitness, both absolute and relative to the population. The value of the function is calculated using each individual's fitness, and the output for each individual is used as that individual's chance to bel selected. Each meta-EA selection function also includes a bit that determines whether or not selected individuals are eligible to be selected again. If this bit is set to False for a particular selection function, then an individual selected by that function will not be selected again in the current generation. The selection functions are evolved in the meta-EA based on the performance of an EA using them. 
The terminals of the GP-Trees include an individual's fitness, as defined by the problem; an individual's fitness-proportion, which is calculated as its own fitness divided by the sum fitness of the population; an individual's fitness rank, defined as the individual's index in a list of all the individuals in the population, sorted by fitness; and numerical constants. Operators in the GP-Tree include mathematical functions (+, -, *, /), the combinatoric function, and the step function. These operators and terminals were chosen because many typical selection functions can be exactly represented by meta-EA individuals with trees built of these elements.
Experimental Setup:
Each selection function is used for 30 runs of the underlying EA with up to 5000 evaluations each run. Selection functions are scored based on the average fitness of the population individuals in the underlying EA. The optimization problems used for the underlying EA are the Rosenbrock function, the Rastrigin function, the 4-bit concatenated D-Trap funciton, and an NK-Landscapes function. For comparison, the EA will also be run using the standard selection functions K-tournament, truncation, fitness proportional, random selection, and fitness ranked for each problem. To show that the selection functions are specializing to the fitness landscape of each problem, the performance of the best selection function for each problem is also measured with an EA run on the other test problems. The hypothesis is that these selection functions will perform worse than or on par with typical selection functions, and worse than the selection function specialized to the particular problem.